{
    "collab_server" : "",
    "contents" : "require(xgboost)\nrequire(methods)\nlibrary(xgboost)\nlibrary(caret)\nlibrary(data.table)\nlibrary(vcd)\nlibrary(e1071)\nlibrary(tidyverse)\nlibrary(Matrix)\n# we load in the agaricus dataset\n# In this example, we are aiming to predict whether a mushroom is edible\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ntrain <- agaricus.train\ntest <- agaricus.test\n\n\n# the loaded data is stored in sparseMatrix, and label is a numeric vector in {0,1}\nclass(train$label)\nclass(train$data)\n\n#-------------Basic Training using XGBoost-----------------\n# this is the basic usage of xgboost you can put matrix in data field\n# note: we are putting in sparse matrix here, xgboost naturally handles sparse input\n# use sparse matrix when your feature is sparse(e.g. when you are using one-hot encoding vector)\nprint(\"Training xgboost with sparseMatrix\")\nbst <- xgboost(data = train$data, label = train$label, max_depth = 2, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\")\n\n# alternatively, you can put in dense matrix, i.e. basic R-matrix\nprint(\"Training xgboost with Matrix\")\nbst <- xgboost(data = as.matrix(train$data), label = train$label, max_depth = 2, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\")\n  \n\n# you can also put in xgb.DMatrix object, which stores label, data and other meta datas needed for advanced features\nprint(\"Training xgboost with xgb.DMatrix\")\ndtrain <- xgb.DMatrix(data = train$data, label = train$label)\nbst <- xgboost(data = dtrain, max_depth = 2, eta = 1, nrounds = 2, nthread = 2, \n               objective = \"binary:logistic\")\n\n\n# Verbose = 0,1,2\nprint(\"Train xgboost with verbose 0, no message\")\nbst <- xgboost(data = dtrain, max_depth = 2, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\", verbose = 0)\nprint(\"Train xgboost with verbose 1, print evaluation metric\")\nbst <- xgboost(data = dtrain, max_depth = 2, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\", verbose = 1)\nprint(\"Train xgboost with verbose 2, also print information about tree\")\nbst <- xgboost(data = dtrain, max_depth = 2, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\", verbose = 2)\n\n# you can also specify data as file path to a LibSVM format input\n# since we do not have this file with us, the following line is just for illustration\n# bst <- xgboost(data = 'agaricus.train.svm', max_depth = 2, eta = 1, nrounds = 2,objective = \"binary:logistic\")\n\n#--------------------basic prediction using xgboost--------------\n# you can do prediction using the following line\n# you can put in Matrix, sparseMatrix, or xgb.DMatrix \npred <- predict(bst, test$data)\nerr <- mean(as.numeric(pred > 0.5) != test$label)\nprint(paste(\"test-error=\", err))\n\n\n#-------------------save and load models-------------------------\n# save model to binary local file\nxgb.save(bst, \"xgboost.model\")\n# load binary model to R\nbst2 <- xgb.load(\"xgboost.model\")\npred2 <- predict(bst2, test$data)\n# pred2 should be identical to pred\nprint(paste(\"sum(abs(pred2-pred))=\", sum(abs(pred2-pred))))\n\n# save model to R's raw vector\nraw = xgb.save.raw(bst)\n# load binary model to R\nbst3 <- xgb.load(raw)\npred3 <- predict(bst3, test$data)\n# pred3 should be identical to pred\nprint(paste(\"sum(abs(pred3-pred))=\", sum(abs(pred3-pred))))\n\n\n#----------------Advanced features --------------\n# to use advanced features, we need to put data in xgb.DMatrix\ndtrain <- xgb.DMatrix(data = train$data, label=train$label)\ndtest <- xgb.DMatrix(data = test$data, label=test$label)\n#---------------Using watchlist----------------\n# watchlist is a list of xgb.DMatrix, each of them is tagged with name\nwatchlist <- list(train=dtrain, test=dtest)\n# to train with watchlist, use xgb.train, which contains more advanced features\n# watchlist allows us to monitor the evaluation result on all data in the list \nprint(\"Train xgboost using xgb.train with watchlist\")\nbst <- xgb.train(data=dtrain, max_depth=2, eta=1, nrounds=2, watchlist=watchlist,\n                 nthread = 2, objective = \"binary:logistic\")\n# we can change evaluation metrics, or use multiple evaluation metrics\nprint(\"train xgboost using xgb.train with watchlist, watch logloss and error\")\nbst <- xgb.train(data=dtrain, max_depth=2, eta=1, nrounds=2, watchlist=watchlist,\n                 eval_metric = \"error\", eval_metric = \"logloss\",\n                 nthread = 2, objective = \"binary:logistic\")\n\n# xgb.DMatrix can also be saved using xgb.DMatrix.save\nxgb.DMatrix.save(dtrain, \"dtrain.buffer\")\n# to load it in, simply call xgb.DMatrix\ndtrain2 <- xgb.DMatrix(\"dtrain.buffer\")\nbst <- xgb.train(data=dtrain2, max_depth=2, eta=1, nrounds=2, watchlist=watchlist,\n                 nthread = 2, objective = \"binary:logistic\")\n# information can be extracted from xgb.DMatrix using getinfo\nlabel = getinfo(dtest, \"label\")\npred <- predict(bst, dtest)\nerr <- as.numeric(sum(as.integer(pred > 0.5) != label))/length(label)\nprint(paste(\"test-error=\", err))\n\n# You can dump the tree you learned using xgb.dump into a text file\nxgb.dump(bst, \"dump.raw.txt\", with_stats = T)\n\n# Finally, you can check which features are the most important.\nprint(\"Most important features (look at column Gain):\")\nimp_matrix <- xgb.importance(feature_names = colnames(train$data), model = bst)\nprint(imp_matrix)\n\n# Feature importance bar plot by gain\nprint(\"Feature importance Plot : \")\nprint(xgb.plot.importance(importance_matrix = imp_matrix))\n\n\n#######################################\n#######################################\nrequire(xgboost)\n# load in the agaricus dataset\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ndtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)\ndtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)\n\nwatchlist <- list(eval = dtest, train = dtrain)\n###\n# advanced: start from a initial base prediction\n#\nprint('start running example to start from a initial prediction')\n# train xgboost for 1 round\nparam <- list(max_depth=2, eta=1, nthread = 2, silent=1, objective='binary:logistic')\nbst <- xgb.train(param, dtrain, 1, watchlist)\n# Note: we need the margin value instead of transformed prediction in set_base_margin\n# do predict with output_margin=TRUE, will always give you margin values before logistic transformation\nptrain <- predict(bst, dtrain, outputmargin=TRUE)\nptest  <- predict(bst, dtest, outputmargin=TRUE)\n# set the base_margin property of dtrain and dtest\n# base margin is the base prediction we will boost from\nsetinfo(dtrain, \"base_margin\", ptrain)\nsetinfo(dtest, \"base_margin\", ptest)\n\nprint('this is result of boost from initial prediction')\nbst <- xgb.train(params = param, data = dtrain, nrounds = 1, watchlist = watchlist)\n\n########################################\n########################################\n# Load Arthritis dataset in memory.\ndata(Arthritis)\n# Create a copy of the dataset with data.table package (data.table is 100% compliant with R dataframe but its syntax is a lot more consistent and its performance are really good).\ndf <- data.table(Arthritis, keep.rownames = F)\n\n# Let's add some new categorical features to see if it helps. Of course these feature are highly correlated to the Age feature. Usually it's not a good thing in ML, but Tree algorithms (including boosted trees) are able to select the best features, even in case of highly correlated features.\n# For the first feature we create groups of age by rounding the real age. Note that we transform it to factor (categorical data) so the algorithm treat them as independant values.\ndf[,AgeDiscret:= as.factor(round(Age/10,0))]\n\n# Here is an even stronger simplification of the real age with an arbitrary split at 30 years old. I choose this value based on nothing. We will see later if simplifying the information based on arbitrary values is a good strategy (I am sure you already have an idea of how well it will work!).\ndf[,AgeCat:= as.factor(ifelse(Age > 30, \"Old\", \"Young\"))]\n\n# We remove ID as there is nothing to learn from this feature (it will just add some noise as the dataset is small).\ndf[,ID:=NULL]\n\n#-------------Basic Training using XGBoost in caret Library-----------------\n# Set up control parameters for caret::train\n# Here we use 10-fold cross-validation, repeating twice, and using random search for tuning hyper-parameters.\nfitControl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 2, search = \"random\")\n# train a xgbTree model using caret::train\nmodel <- train(factor(Improved)~., data = df, method = \"xgbTree\", trControl = fitControl)\n\n# Instead of tree for our boosters, you can also fit a linear regression or logistic regression model using xgbLinear\n# model <- train(factor(Improved)~., data = df, method = \"xgbLinear\", trControl = fitControl)\n\n# See model results\nprint(model)\n\n############################################\n############################################\n# load in the agaricus dataset\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ndtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)\ndtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)\n##\n#  this script demonstrate how to fit generalized linear model in xgboost\n#  basically, we are using linear model, instead of tree for our boosters\n#  you can fit a linear regression, or logistic regression model\n##\n\n# change booster to gblinear, so that we are fitting a linear model\n# alpha is the L1 regularizer \n# lambda is the L2 regularizer\n# you can also set lambda_bias which is L2 regularizer on the bias term\nparam <- list(objective = \"binary:logistic\", booster = \"gblinear\",\n              nthread = 2, alpha = 0.0001, lambda = 1)\n\n# normally, you do not need to set eta (step_size)\n# XGBoost uses a parallel coordinate descent algorithm (shotgun), \n# there could be affection on convergence with parallelization on certain cases\n# setting eta to be smaller value, e.g 0.5 can make the optimization more stable\n\n##\n# the rest of settings are the same\n##\nwatchlist <- list(eval = dtest, train = dtrain)\nnum_round <- 2\nbst <- xgb.train(param, dtrain, num_round, watchlist)\nypred <- predict(bst, dtest)\nlabels <- getinfo(dtest, 'label')\ncat('error of preds=', mean(as.numeric(ypred>0.5)!=labels),'\\n')\n\n##################################################\n##################################################\nset.seed(1982)\n\n# load in the agaricus dataset\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ndtrain <- xgb.DMatrix(data = agaricus.train$data, label = agaricus.train$label)\ndtest <- xgb.DMatrix(data = agaricus.test$data, label = agaricus.test$label)\n\nparam <- list(max_depth=2, eta=1, silent=1, objective='binary:logistic')\nnround = 4\n\n# training the model for two rounds\nbst = xgb.train(params = param, data = dtrain, nrounds = nround, nthread = 2)\n\n# Model accuracy without new features\naccuracy.before <- sum((predict(bst, agaricus.test$data) >= 0.5) == agaricus.test$label) / length(agaricus.test$label)\n\n# by default, we predict using all the trees\n\npred_with_leaf = predict(bst, dtest, predleaf = TRUE)\nhead(pred_with_leaf)\n\ncreate.new.tree.features <- function(model, original.features){\n  pred_with_leaf <- predict(model, original.features, predleaf = TRUE)\n  cols <- list()\n  for(i in 1:model$niter){\n    # max is not the real max but it s not important for the purpose of adding features\n    leaf.id <- sort(unique(pred_with_leaf[,i]))\n    cols[[i]] <- factor(x = pred_with_leaf[,i], level = leaf.id)\n  }\n  cBind(original.features, sparse.model.matrix( ~ . -1, as.data.frame(cols)))\n}\n\n# Convert previous features to one hot encoding\nnew.features.train <- create.new.tree.features(bst, agaricus.train$data)\nnew.features.test <- create.new.tree.features(bst, agaricus.test$data)\n\n# learning with new features\nnew.dtrain <- xgb.DMatrix(data = new.features.train, label = agaricus.train$label)\nnew.dtest <- xgb.DMatrix(data = new.features.test, label = agaricus.test$label)\nwatchlist <- list(train = new.dtrain)\nbst <- xgb.train(params = param, data = new.dtrain, nrounds = nround, nthread = 2)\n\n# Model accuracy with new features\naccuracy.after <- sum((predict(bst, new.dtest) >= 0.5) == agaricus.test$label) / length(agaricus.test$label)\n\n# Here the accuracy was already good and is now perfect.\ncat(paste(\"The accuracy was\", accuracy.before, \"before adding leaf features and it is now\", accuracy.after, \"!\\n\"))",
    "created" : 1504007659824.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "4042695709",
    "id" : "50946D5E",
    "lastKnownWriteTime" : 3186353048573255724,
    "last_content_update" : 1504008760525,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled6"
    },
    "relative_order" : 13,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}