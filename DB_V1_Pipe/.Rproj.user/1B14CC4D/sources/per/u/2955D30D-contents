# Load the H2O library and start up the H2O cluster locally on your machine
library(h2o)
h2o.init(nthreads = -1, #Number of threads -1 means use all cores on your machine
         max_mem_size = "8G")  #max mem size is the maximum memory to allocate to H2O



# Next we will import a cleaned up version of the Lending Club "Bad Loans" dataset
# The purpose here is to predict whether a loan will be bad (not repaid to the lender)
# The response column, bad_loan, is 1 if the loan was bad, and 0 otherwise

# Import the data
# loan_csv <- "/Volumes/H2OTOUR/loan.csv"  # modify this for your machine
# Alternatively, you can import the data directly from a URL
loan_csv <- "https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv"
data <- h2o.importFile(loan_csv)  # 163,987 rows x 15 columns
dim(data)
# Since we want to train a binary classification model, 
# we must ensure that the response is coded as a factor
# If the response is 0/1, H2O will assume it's numeric,
# which means that H2O will train a regression model instead
data$bad_loan <- as.factor(data$bad_loan)  #encode the binary repsonse as a factor
h2o.levels(data$bad_loan)  #optional: after encoding, this shows the two factor levels, '0' and '1'

# Partition the data into training, validation and test sets
splits <- h2o.splitFrame(data = data, 
                         ratios = c(0.7, 0.15),  #partition data into 70%, 15%, 15% chunks
                         seed = 1)  #setting a seed will guarantee reproducibility

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]


# Take a look at the size of each partition
# Notice that h2o.splitFrame uses approximate splitting not exact splitting (for efficiency)
# so these are not exactly 70%, 15% and 15% of the total rows
nrow(train)  # 114908
nrow(valid) # 24498
nrow(test)  # 24581


# Identify response and predictor variables
y <- "bad_loan"
x <- setdiff(names(data), c(y, "int_rate"))  #remove the interest rate column because it's correlated with the outcome

# Now that we have prepared the data, we can train some models
# We will start by training a single model from each of the H2O supervised algos:
# 1. Generalized Linear Model (GLM)
# 2. Random Forest (RF)
# 3. Gradient Boosting Machine (GBM)
# 4. Deep Learning (DL)
# 5. Naive Bayes (NB)

# 1. Let's start with a basic binomial Generalized Linear Model
# By default, h2o.glm uses a regularized, elastic net model
  glm_fit1 <- h2o.glm(x = x, 
                      y = y, 
                      training_frame = train,
                      model_id = "glm_fit1",
                      family = "binomial")  #similar to R's glm, h2o.glm has the family argument

  
  # Next we will do some automatic tuning by passing in a validation frame and setting 
  # `lambda_search = True`.  Since we are training a GLM with regularization, we should 
  # try to find the right amount of regularization (to avoid overfitting).  The model 
  # parameter, `lambda`, controls the amount of regularization in a GLM model and we can 
  # find the optimal value for `lambda` automatically by setting `lambda_search = TRUE` 
  # and passing in a validation frame (which is used to evaluate model performance using a 
  # particular value of lambda).
  glm_fit2 <- h2o.glm(x = x, 
                      y = y, 
                      training_frame = train,
                      model_id = "glm_fit2",
                      validation_frame = valid,
                      family = "binomial",
                      lambda_search = TRUE)
  
  # Let's compare the performance of the two GLMs
  glm_perf1 <- h2o.performance(model = glm_fit1,
                               newdata = test)
  glm_perf2 <- h2o.performance(model = glm_fit2,
                               newdata = test)
  
  # Print model performance
  glm_perf1
  glm_perf2
  
  # Instead of printing the entire model performance metrics object, 
  # it is probably easier to print just the metric that you are interested in comparing.
  # Retreive test set AUC
  h2o.auc(glm_perf1)  #0.677449084114
  h2o.auc(glm_perf2)  #0.677675858276
  
  
  
  # Compare test AUC to the training AUC and validation AUC
  h2o.auc(glm_fit2, train = TRUE)  #0.674306164325 
  h2o.auc(glm_fit2, valid = TRUE)  #0.675512216705
  glm_fit2@model$validation_metrics  #0.675512216705
  
  
  
  # 2. Random Forest
  # H2O's Random Forest (RF) implements a distributed version of the standard 
  # Random Forest algorithm and variable importance measures.
  # First we will train a basic Random Forest model with default parameters. 
  # The Random Forest model will infer the response distribution from the response encoding. 
  # A seed is required for reproducibility.
  rf_fit1 <- h2o.randomForest(x = x,
                              y = y,
                              training_frame = train,
                              model_id = "rf_fit1",
                              seed = 1)
  
  # Next we will increase the number of trees used in the forest by setting `ntrees = 100`.  
  # The default number of trees in an H2O Random Forest is 50, so this RF will be twice as 
  # big as the default.  Usually increasing the number of trees in a RF will increase 
  # performance as well.  Unlike Gradient Boosting Machines (GBMs), Random Forests are fairly 
  # resistant (although not free from) overfitting.
  # See the GBM example below for additional guidance on preventing overfitting using H2O's 
  # early stopping functionality.
  rf_fit2 <- h2o.randomForest(x = x,
                              y = y,
                              training_frame = train,
                              model_id = "rf_fit2",
                              #validation_frame = valid,  #only used if stopping_rounds > 0
                              ntrees = 100,
                              seed = 1)
  
  # Let's compare the performance of the two RFs
  rf_perf1 <- h2o.performance(model = rf_fit1,
                              newdata = test)
  rf_perf2 <- h2o.performance(model = rf_fit2,
                              newdata = test)
  
  # Print model performance
  rf_perf1
  rf_perf2